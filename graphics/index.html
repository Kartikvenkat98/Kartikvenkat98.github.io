<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/themes/prism.min.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css">
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>
</head>

<body>
  <div id="project">CSCE 641 COMPUTER GRAPHICS PROJECT</div>
  <div id="implement">IMPLEMENTATION OF RESEARCH PAPER</div>
  <div id="title">EFFICIENT SURFEL-BASED SLAM USING 3D LASER RANGE DATA IN URBAN ENVIRONMENTS</div>
  <div id="name">Kartik Venkataraman</div>
  <div id="toUpdate" align="center">
  <a href="#update">Click here to see the project update report (11/20/2020)</a>
  </div>
  <div id="example">
    <img src="example.svg">
  </div>

  <div align="center">
  <h1>PROJECT PROPOSAL</h1>
  </div>

  <h2>SUMMARY:</h2>

  <div id="block">
    <h3>Description</h3>
    <p>For the CSCE 641 Computer Graphics project, I would be working on implementing the research paper - Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments. This paper proposes a dense approach to laser-based mapping that operates on three-dimensional point clouds obtained from rotating laser sensors. A surfel-based map is constructed and it estimates the changes in the robot's pose by exploiting the projective data association between the current scan and a rendered model view from that surfel map. For detection and verification of a loop closure, the authors have leveraged the map representation to compose a virtual view of the map before a potential loop closure, which enables a more robust detection even with low overlap between the scan and the already mapped areas. This approach is efficient and enables real-time capable registration. At the same time, it is able to detect loop closures and to perform map updates in an online fashion. The experiments performed show that it is possible to estimate globally consistent maps in large scale environments solely based on point cloud data.
    </p>
  </div>

  <div id="block">
    <h3>Importance</h3>
    <p>Accurate and reliable localization and mapping is a fundamental building block for most autonomous robots. Most autonomous robots, including self-driving cars, must be able to reliably localize themselves, ideally by using only their own sensors without relying on external information such as GPS or other additional infrastructure placed in the environment. Dense approaches have a prospective advantage over feature-based and sparse approaches as they use all available information and thus do not depend on reliable feature extraction or availability of such features.</p>
  </div>

  <div id="block">
    <h3>Previous Work</h3>
    <p>Odometry estimation and Simultaneous Localization and Mapping (SLAM), especially with (stereo) cameras and 2D laser range scanners, is a classical topic in robotics and in the computer vision community. There has been significant advances in visionbased and RGB-D-based SLAM systems over the past few years. Most of these approaches use (semi-)dense reconstructions of the environment and exploit them for frameto-model tracking, either by jointly optimizing the map and pose estimates or by alternating pose estimation and map building.</p>
    <p> The current 3D laser-based mapping systems mainly accomplish the estimation relying on feature-based solutions, reduced map representations, voxel grid-based methods, or point sub-sampling, which all effectively reduce the data used for alignment.</p>
    <p>There has also been considerable progress in the field of RGB-D-based SLAM. KinectFusion, largely impacted the development in the subsequent years. In line with this, another approach of RGB-D SLAM uses projective data association in a dense model, but relies on a surfel-based map for tracking. The surfel-based representation allows to model comparably large environments without a compromise in terms of reduced reconstruction fidelity due to reduced grid resolution or a more complex implementation. Other recent RGB-D approaches use the surfels to extract planar surfaces or perform online loop closure integration by deforming the surfel representation.</p>
    <p>For more information on previous related work done on this topic, please have a look at the research paper, link to which is provided below.
  </div>

  <div id="links">
    <a href="behley2018rss.pdf" target="blank">
      <i class="far fa-file-pdf"></i> PDF version of research paper</a>
  </div>

  <div id="block">
    <h3>Proposal</h3>
    <p>I'm really interested in Robotics and Computer Vision which led me to this paper in the first place. </p>
    <p> First, I would like to go through more related work on SLAM techniques and learn more about the topic.</p>
    <p>Then I would learn more about ROS (Robot Operating System). ROS is an open-source, meta-operating system for your robot. It provides the services you would expect from an operating system, including hardware abstraction, low-level device control, implementation of commonly-used functionality, message-passing between processes, and package management. It also provides tools and libraries for obtaining, building, writing, and running code across multiple computers.</p>
    <p>After learning about the required softwares and tools, I would start implementing this research paper.</p>
  </div>

  <div id="block">
    <h3>Originality</h3>
    <p>No, this is not my original work. I'm just interested in this research paper and found it to be challenging. So, I'm planning to implement this paper. I have cited this reseach paper below.</p>
  </div>

    <div id="block">
    <h3>Relationship to Graphics</h3>
    <p>I'm doing my paper presentation on Point-based Rendering/Surfels. I wanted to work on something related to Surfels and as I'm interested in Robotics, I chose this paper.</p>
  </div>

  <h2>GOALS:</h2>

  <div id="block">
    <h3>Intermediate Goals</h3>
    <ol>
      <li>Learn more about ROS</li>
      <li>Install ROS and required dependencies such as catkin</li>
      <li>Install OpenGL and required dependencies</li>
      <li>Have a basic workspace up and running</li>
    </ol>

    <h3>Final Goals</h3>
    <ol>
      <li>Have a fully working implementation of this research paper.</li>
      <li>If I have some time left, try something new as a stretch goal.</li>
    </ol>
  </div>

  <div id="update" align="center">
  <h1>UPDATED REPORT</h1>
  </div>

  <h2>SUMMARY:</h2>

  <div id="block">
    <p>I went through the paper and read upon related terms and tried to understand the underlying concept. I learnt more about ROS (Robot Operating System). I installed the required dependencies for ROS such as catkin and created a catkin workspace.  I also installed OpenGL and required external libraries. So, now I have a basic workspace up and running.
    </p>
  </div>

  <h2>ANALYSIS:</h2>

  <div id="block">
  <h3>Robot Operating System</h3>
    <p>ROS is an open-source, meta-operating system for a robot. It provides the services you would expect from an operating system, including hardware abstraction, low-level device control, implementation of commonly-used functionality, message-passing between processes, and package management. It also provides tools and libraries for obtaining, building, writing, and running code across multiple computers and more. ROS is licensed under an open source, BSD license.
    </p>
    <p>The ROS runtime "graph" is a peer-to-peer network of processes (potentially distributed across machines) that are loosely coupled using the ROS communication infrastructure. ROS implements several different styles of communication, including synchronous RPC-style communication over services, asynchronous streaming of data over topics, and storage of data on a Parameter Server.
    </p>
    <p>ROS currently only runs on Unix-based platforms. Software for ROS is primarily tested on Ubuntu and Mac OS X systems, though the ROS community has been contributing support for Fedora, Gentoo, Arch Linux and other Linux platforms. ROS framework is easy to implement in any modern programming language, Python, C++, Java, etc. It can be easily integrated with OpenCV.
    </p>
    <div align="center">
    <a href="http://wiki.ros.org/">Click here to learn more about ROS</a>
    </div>

  <h3>Catkin</h3>
    <p>Catkin is the official build system of ROS. catkin combines CMake macros and Python scripts to provide some functionality on top of CMake's normal workflow. catkin was designed to allow better distribution of packages, better cross-compiling support, and better portability. catkin's workflow is very similar to CMake's but adds support for automatic 'find package' infrastructure and building multiple, dependent projects at the same time.
    </p>
    <div align="center">
      <a href="http://wiki.ros.org/catkin">Click here to learn more about catkin</a>
    </div>

  <h3>Installing Dependencies</h3>
   <ol>
      <li>catkin</li>
      <li>Qt5 >= 5.2.1</li>
      <li>OpenGL >= 3.3</li>
      <li>libEigen >= 3.2</li>
      <li>gtsam >= 4.0</li>
    </ol>
  <p>In Ubuntu 16.04, installing all dependencies can be accomplished by</p>
  <em>sudo apt-get install build-essential cmake libgtest-dev libeigen3-dev libboost-all-dev qtbase5-dev libglew-dev libqt5libqgtk2 catkin</em></br>
  <p>Additionally, we need to have catkin-tools and the fetch verb installed.</p>
  <em>sudo apt install python-pip</em></br>
  <em>sudo pip install catkin_tools catkin_tools_fetch empy</em></br>
  <p>We need to create a catkin workspace</p>
  <em>cd</em></br>
  <em>mkdir -p catkin_ws/src && cd catkin_ws</em></br>
  <em>catkin init</em></br>
  <em>cd src && git clone https://github.com/ros/catkin.git</em></br>
  </div>

  <h2>PLAN AHEAD:</h2>

  <div id="block">
    <p>I will start implementing the paper according to the steps mentioned in the paper.
     <ol>
      <li>Preprocessing - For projective data association, we first project the point cloud P to the vertex map Vd : R2 -> R3 , where each pixel contains the nearest 3D point.</li>
      <li>Map Representation - A surfel-based map is employed since it allows us to represent even large-scale environments and maintain dense, detailed geometric information of the point clouds at the same time</li>
      <li>Odometry Estimation - The observerd point cloud is aligned to a rendered representation of the model in coordinate frame before rendering all surfels, resulting in a local view of the already mapped world at timestmap tâˆ’1.</li>
      <li>Map Update - Given the current pose of the frame-to-model ICP, we integrate the points inside into the surfel map.</li>
      <li>Loop Closures - For loop closure detection, we exploit the surfel map to render views Analogous to the odometry, we now use this rendered view to register the current point cloud. A loop closure is only considered if a composed virtual map view built from this alignment fits to the current measurements.</li>
    </ol>
    </p>
  </div>





  <h2>CITATION:</h2>

  <div id="block">    
  <p>J. Behley, C. Stachniss. Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments, Proc. of Robotics: Science and Systems (RSS), 2018.</p>
  </div>

  <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/prism.min.js"></script>
</body>

</html>